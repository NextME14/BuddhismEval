{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp43pXkl6B+y03QvqdjhJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NextME14/BuddhismEval/blob/main/BuddhismEval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "LDaFQPQWhmqy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQEzB18LgVAA"
      },
      "outputs": [],
      "source": [
        "#  openpyxl for Excel outputs\n",
        "!pip install --quiet requests pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Your API Key"
      ],
      "metadata": {
        "id": "lX0UyveAhvYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace this with your real key (keep it secret!)\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"API-KEY\"\n"
      ],
      "metadata": {
        "id": "YrUaPpiqhwrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Your JSONL"
      ],
      "metadata": {
        "id": "bsu6nx7zh8T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # click “Choose Files” and pick your .jsonl\n",
        "# after upload, /content/english_eval.jsonl will exist\n"
      ],
      "metadata": {
        "id": "p5GEDhhkh8Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluator"
      ],
      "metadata": {
        "id": "hROYIz_AiZxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, os, pandas as pd\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "\n",
        "# ─── CONFIG ─────────────────────\n",
        "API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "DATA_PATH       = \"/content/english_eval.jsonl\"\n",
        "OUTPUT_TEMPLATE = \"/content/English_evaluation_claud_{mode}prompt.xlsx\"\n",
        "# ─────────────────────────────────\n",
        "\n",
        "def init_results():\n",
        "    return {\n",
        "        'questions': [],\n",
        "        'correct': defaultdict(int),\n",
        "        'incorrect': defaultdict(int),\n",
        "        'missed': defaultdict(int),\n",
        "        'q_label_breakdown': defaultdict(lambda: {'correct':0,'incorrect':0,'missed':0}),\n",
        "        'start_time': time.time()\n",
        "    }\n",
        "\n",
        "def load_data(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"[load_data] skipping bad line: {e}\")\n",
        "    return data\n",
        "\n",
        "def apply_prompt(entry):\n",
        "    return (\n",
        "        \"[INST] You are a Theravada Buddhist scholar with deep knowledge of the Dhammapada.\\n\"\n",
        "        \"Analyze the question step-by-step internally, but only output the final answer (1, 2, or 3).\\n\\n\"\n",
        "        f\"Question: {entry['question']}\\n\\n\"\n",
        "        \"Options:\\n\"\n",
        "        f\"1. {entry['options'][0]}\\n\"\n",
        "        f\"2. {entry['options'][1]}\\n\"\n",
        "        f\"3. {entry['options'][2]}\\n\\n\"\n",
        "        \"Your final answer (1, 2, or 3): [/INST]\"\n",
        "    )\n",
        "\n",
        "def send_api_request(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"anthropic/claude-3-haiku\",\n",
        "        \"messages\": [{\"role\":\"user\",\"content\":prompt}],\n",
        "        \"temperature\":0.2,\n",
        "        \"max_tokens\":150\n",
        "    }\n",
        "    try:\n",
        "        r = requests.post(API_URL, headers=headers, json=payload)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "    except Exception as e:\n",
        "        print(f\"[API error] {e}\")\n",
        "        return None\n",
        "\n",
        "def predict_option(entry):\n",
        "    prompt = apply_prompt(entry)\n",
        "    resp   = send_api_request(prompt)\n",
        "    if resp and resp.get(\"choices\"):\n",
        "        txt = resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "        for opt in ('1','2','3'):\n",
        "            if txt.startswith(opt):\n",
        "                return int(opt), txt\n",
        "    return None, None\n",
        "\n",
        "def process_question(item, results):\n",
        "    label = str(item.get('q_label','Unknown'))\n",
        "    detail = {\n",
        "        'id': item['id'],\n",
        "        'question': item['question'],\n",
        "        'options': item['options'],\n",
        "        'correct_answer': item['correct_answer'],\n",
        "        'q_label': label,\n",
        "        'response': None,\n",
        "        'result': 'missed',\n",
        "        'reasoning': ''\n",
        "    }\n",
        "\n",
        "    # find correct index\n",
        "    try:\n",
        "        correct_idx = item['options'].index(item['correct_answer']) + 1\n",
        "    except ValueError:\n",
        "        results['missed']['total'] += 1\n",
        "        results['q_label_breakdown'][label]['missed'] += 1\n",
        "        results['questions'].append(detail)\n",
        "        return\n",
        "\n",
        "    pred, reasoning = predict_option(item)\n",
        "    detail['reasoning'] = reasoning or \"No response\"\n",
        "    if pred is None:\n",
        "        results['missed']['total'] += 1\n",
        "        results['q_label_breakdown'][label]['missed'] += 1\n",
        "    else:\n",
        "        detail['response'] = pred\n",
        "        if pred == correct_idx:\n",
        "            detail['result'] = 'correct'\n",
        "            results['correct']['total'] += 1\n",
        "            results['q_label_breakdown'][label]['correct'] += 1\n",
        "        else:\n",
        "            detail['result'] = 'incorrect'\n",
        "            results['incorrect']['total'] += 1\n",
        "            results['q_label_breakdown'][label]['incorrect'] += 1\n",
        "\n",
        "    results['questions'].append(detail)\n",
        "\n",
        "def generate_report(path, results):\n",
        "    df_det = pd.DataFrame(results['questions'])\n",
        "    total  = len(results['questions'])\n",
        "    summary = {\n",
        "        'Metric': [\"Total\",\"Correct\",\"Incorrect\",\"Missed\",\"Accuracy\",\"Time(s)\"],\n",
        "        'Value': [\n",
        "            total,\n",
        "            results['correct']['total'],\n",
        "            results['incorrect']['total'],\n",
        "            results['missed']['total'],\n",
        "            f\"{results['correct']['total']/total*100:.2f}%\",\n",
        "            f\"{time.time()-results['start_time']:.2f}\"\n",
        "        ]\n",
        "    }\n",
        "    df_sum = pd.DataFrame(summary)\n",
        "    breakdown=[]\n",
        "    for lbl,st in sorted(results['q_label_breakdown'].items()):\n",
        "        c,i,m = st['correct'],st['incorrect'],st['missed']\n",
        "        tot   = c+i+m\n",
        "        breakdown.append({\n",
        "            'Label':lbl,\n",
        "            'Correct':c,'Incorrect':i,'Missed':m,\n",
        "            'Accuracy':f\"{c/tot*100:.2f}%\" if tot else \"N/A\"\n",
        "        })\n",
        "    df_bd = pd.DataFrame(breakdown)\n",
        "\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with pd.ExcelWriter(path, engine='openpyxl') as w:\n",
        "        df_det.to_excel(w, sheet_name='Details', index=False)\n",
        "        df_sum.to_excel(w, sheet_name='Summary', index=False)\n",
        "        df_bd.to_excel(w, sheet_name='Label Analysis', index=False)\n",
        "\n",
        "def evaluate():\n",
        "    print(\" Loading data…\")\n",
        "    data = load_data(DATA_PATH)\n",
        "    print(f\"→ {len(data)} questions loaded.\")\n",
        "\n",
        "    results = init_results()\n",
        "    for idx, item in enumerate(data,1):\n",
        "        process_question(item, results)\n",
        "        if idx%10==0:\n",
        "            print(f\"  • {idx}/{len(data)} done\")\n",
        "\n",
        "    out = OUTPUT_TEMPLATE.format(mode=\"long\")\n",
        "    generate_report(out, results)\n",
        "    print(f\" Report saved to {out}\")\n",
        "    return out\n",
        "\n",
        "# run it\n",
        "if __name__==\"__main__\":\n",
        "    evaluate()\n"
      ],
      "metadata": {
        "id": "hqEeI3FOicFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Your Report"
      ],
      "metadata": {
        "id": "8mf1DQY2i8Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/English_evaluation_claud_longprompt.xlsx\")\n"
      ],
      "metadata": {
        "id": "sIA_2MnGi9iY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}